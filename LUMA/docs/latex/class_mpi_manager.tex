\hypertarget{class_mpi_manager}{}\section{Mpi\+Manager Class Reference}
\label{class_mpi_manager}\index{Mpi\+Manager@{Mpi\+Manager}}


M\+PI Manager class.  




{\ttfamily \#include $<$Mpi\+Manager.\+h$>$}

\subsection*{Classes}
\begin{DoxyCompactItemize}
\item 
struct \hyperlink{struct_mpi_manager_1_1buffer__struct}{buffer\+\_\+struct}
\begin{DoxyCompactList}\small\item\em Structure storing buffers sizes in each direction for particular grid. \end{DoxyCompactList}\item 
struct \hyperlink{struct_mpi_manager_1_1layer__edges}{layer\+\_\+edges}
\begin{DoxyCompactList}\small\item\em Structure containing absolute positions of the edges of halos. \end{DoxyCompactList}\item 
struct \hyperlink{struct_mpi_manager_1_1phdf5__struct}{phdf5\+\_\+struct}
\begin{DoxyCompactList}\small\item\em Structure for storing halo information for H\+D\+F5. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
void \hyperlink{class_mpi_manager_a02adaa06e139dfca2bc71e1a1dbf25c7}{mpi\+\_\+init} ()
\begin{DoxyCompactList}\small\item\em Initialisation routine. \end{DoxyCompactList}\item 
void \hyperlink{class_mpi_manager_a7f07e85131147b55eec643c791ec2ba0}{mpi\+\_\+gridbuild} ()
\begin{DoxyCompactList}\small\item\em Domain decomposition. \end{DoxyCompactList}\item 
int \hyperlink{class_mpi_manager_a749fa958cb7343183a69ca6191b45286}{mpi\+\_\+build\+Communicators} ()
\begin{DoxyCompactList}\small\item\em Define writable sub-\/grid communicators. \end{DoxyCompactList}\item 
void \hyperlink{class_mpi_manager_ae6a8b1a857a00cdc4ed115512db050f4}{mpi\+\_\+update\+Load\+Info} ()
\begin{DoxyCompactList}\small\item\em Update the load balancing information stored in the \hyperlink{class_mpi_manager}{Mpi\+Manager}. \end{DoxyCompactList}\item 
void \hyperlink{class_mpi_manager_ae1b33a4a24d9abf528528a296aa1d92d}{mpi\+\_\+buffer\+\_\+pack} (int dir, \hyperlink{class_grid_obj}{Grid\+Obj} $\ast$g)
\begin{DoxyCompactList}\small\item\em Method to pack the communication buffer. \end{DoxyCompactList}\item 
void \hyperlink{class_mpi_manager_abf5e0511918b4ae6ec524d737618e341}{mpi\+\_\+buffer\+\_\+unpack} (int dir, \hyperlink{class_grid_obj}{Grid\+Obj} $\ast$g)
\begin{DoxyCompactList}\small\item\em Method to unpack the communication buffer. \end{DoxyCompactList}\item 
void \hyperlink{class_mpi_manager_a1bf713399e26a5ffbc03147e0a20c585}{mpi\+\_\+buffer\+\_\+size} ()
\begin{DoxyCompactList}\small\item\em Pre-\/calcualtion of the buffer sizes. \end{DoxyCompactList}\item 
void \hyperlink{class_mpi_manager_af26d2a0a2430f7c1b5def5f954a10f1d}{mpi\+\_\+buffer\+\_\+size\+\_\+send} (\hyperlink{class_grid_obj}{Grid\+Obj} $\ast$\&g)
\begin{DoxyCompactList}\small\item\em Method to pre-\/compute the size of the sender layer buffer. \end{DoxyCompactList}\item 
void \hyperlink{class_mpi_manager_afa7547c05583bf6c52ea48cc1dc13336}{mpi\+\_\+buffer\+\_\+size\+\_\+recv} (\hyperlink{class_grid_obj}{Grid\+Obj} $\ast$\&g)
\begin{DoxyCompactList}\small\item\em Method to pre-\/compute the size of the receiver layer buffer. \end{DoxyCompactList}\item 
void \hyperlink{class_mpi_manager_ab498bdf0822e2747f83c187d682dd934}{mpi\+\_\+writeout\+\_\+buf} (std\+::string filename, int dir)
\begin{DoxyCompactList}\small\item\em Buffer A\+S\+C\+II writer. \end{DoxyCompactList}\item 
void \hyperlink{class_mpi_manager_aedcf84c06fc3e0486fac61d09ce0a268}{mpi\+\_\+communicate} (int level, int regnum)
\begin{DoxyCompactList}\small\item\em Communication routine. \end{DoxyCompactList}\item 
int \hyperlink{class_mpi_manager_a3c10ab477c2e4387d6a02104f9b2a2ea}{mpi\+\_\+get\+Opposite} (int direction)
\begin{DoxyCompactList}\small\item\em Helper method to find opposite direction in M\+PI topology. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Static Public Member Functions}
\begin{DoxyCompactItemize}
\item 
static \hyperlink{class_mpi_manager}{Mpi\+Manager} $\ast$ \hyperlink{class_mpi_manager_a486e424ae1b9dfa3218d260b0f9a0a2f}{get\+Instance} ()
\begin{DoxyCompactList}\small\item\em Instance creator. \end{DoxyCompactList}\item 
static void \hyperlink{class_mpi_manager_a03b7914615ccb6e7b8a285f50860d503}{destroy\+Instance} ()
\begin{DoxyCompactList}\small\item\em Instance destroyer. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
M\+P\+I\+\_\+\+Comm \hyperlink{class_mpi_manager_aec1ed834d1a8fa19f87499fb0d5cd332}{world\+\_\+comm}
\begin{DoxyCompactList}\small\item\em Global M\+PI communicator. \end{DoxyCompactList}\item 
int \hyperlink{class_mpi_manager_a8d486f77671328cdc139f6cef2a4006f}{dimensions} \mbox{[}\hyperlink{definitions_8h_a31d5945080ee5c34edc32e6f74c724c8}{L\+\_\+\+D\+I\+MS}\mbox{]}
\begin{DoxyCompactList}\small\item\em Size of M\+PI Cartesian topology. \end{DoxyCompactList}\item 
int \hyperlink{class_mpi_manager_af2891954ff504c12ec6d5f845e906f28}{neighbour\+\_\+rank} \mbox{[}\hyperlink{definitions_8h_a144328eed4e90ebcf8a9f66aa7337266}{L\+\_\+\+M\+P\+I\+\_\+\+D\+I\+RS}\mbox{]}
\begin{DoxyCompactList}\small\item\em Neighbour rank number for each direction in Cartesian topology. \end{DoxyCompactList}\item 
int \hyperlink{class_mpi_manager_a5a7268347fcab916adc61bee47e9f626}{neighbour\+\_\+coords} \mbox{[}\hyperlink{definitions_8h_a31d5945080ee5c34edc32e6f74c724c8}{L\+\_\+\+D\+I\+MS}\mbox{]}\mbox{[}\hyperlink{definitions_8h_a144328eed4e90ebcf8a9f66aa7337266}{L\+\_\+\+M\+P\+I\+\_\+\+D\+I\+RS}\mbox{]}
\begin{DoxyCompactList}\small\item\em Coordinates in M\+PI topology of neighbour ranks. \end{DoxyCompactList}\item 
M\+P\+I\+\_\+\+Comm \hyperlink{class_mpi_manager_a0926101699de914f6be018885bea25b1}{sub\+Grid\+\_\+comm} \mbox{[}\hyperlink{definitions_8h_a2ce7c3facc5f789b0e201757516539a5}{L\+\_\+\+N\+U\+M\+\_\+\+L\+E\+V\+E\+LS} $\ast$\hyperlink{definitions_8h_a3efeae83589481193d81da498e7f746a}{L\+\_\+\+N\+U\+M\+\_\+\+R\+E\+G\+I\+O\+NS}\mbox{]}
\begin{DoxyCompactList}\small\item\em Communicators for sub-\/grid / region combinations. \end{DoxyCompactList}\item 
std\+::vector$<$ \hyperlink{struct_mpi_manager_1_1phdf5__struct}{phdf5\+\_\+struct} $>$ \hyperlink{class_mpi_manager_a03972530e718d5b0a7f119e9c6132179}{p\+\_\+data}
\begin{DoxyCompactList}\small\item\em Vector of structures containing halo descriptors for block writing (H\+D\+F5) \end{DoxyCompactList}\item 
int \hyperlink{class_mpi_manager_a8329212abc23e5fa3e32e961b7823b5b}{my\+\_\+rank}
\begin{DoxyCompactList}\small\item\em Rank number. \end{DoxyCompactList}\item 
int \hyperlink{class_mpi_manager_af5156a5e4519f43230b6b84792464e48}{num\+\_\+ranks}
\begin{DoxyCompactList}\small\item\em Total number of ranks in M\+PI Cartesian topology. \end{DoxyCompactList}\item 
int \hyperlink{class_mpi_manager_a54a3ad1d90d1508ebc82f81655d917f8}{rank\+\_\+coords} \mbox{[}\hyperlink{definitions_8h_a31d5945080ee5c34edc32e6f74c724c8}{L\+\_\+\+D\+I\+MS}\mbox{]}
\begin{DoxyCompactList}\small\item\em Coordinates in M\+PI Cartesian topology. \end{DoxyCompactList}\item 
int \hyperlink{class_mpi_manager_a3cdf6e1ce19f22daa9e84bc88bf4382d}{global\+\_\+size} \mbox{[}3\mbox{]}\mbox{[}\hyperlink{definitions_8h_a2ce7c3facc5f789b0e201757516539a5}{L\+\_\+\+N\+U\+M\+\_\+\+L\+E\+V\+E\+LS} $\ast$\hyperlink{definitions_8h_a3efeae83589481193d81da498e7f746a}{L\+\_\+\+N\+U\+M\+\_\+\+R\+E\+G\+I\+O\+NS}+1\mbox{]}
\begin{DoxyCompactList}\small\item\em Overall size of each grid (excluding halo of course). \end{DoxyCompactList}\item 
double \hyperlink{class_mpi_manager_a26f0512e19009451431d6d0ba59bf81a}{global\+\_\+edges} \mbox{[}6\mbox{]}\mbox{[}\hyperlink{definitions_8h_a2ce7c3facc5f789b0e201757516539a5}{L\+\_\+\+N\+U\+M\+\_\+\+L\+E\+V\+E\+LS} $\ast$\hyperlink{definitions_8h_a3efeae83589481193d81da498e7f746a}{L\+\_\+\+N\+U\+M\+\_\+\+R\+E\+G\+I\+O\+NS}+1\mbox{]}
\begin{DoxyCompactList}\small\item\em Absolute position of grid edges (excluding halo of course). \end{DoxyCompactList}\item 
bool \hyperlink{class_mpi_manager_a18d53c4f9968cccec36127d33776e3d4}{subgrid\+\_\+tlayer\+\_\+key} \mbox{[}6\mbox{]}\mbox{[}\hyperlink{definitions_8h_a2ce7c3facc5f789b0e201757516539a5}{L\+\_\+\+N\+U\+M\+\_\+\+L\+E\+V\+E\+LS} $\ast$\hyperlink{definitions_8h_a3efeae83589481193d81da498e7f746a}{L\+\_\+\+N\+U\+M\+\_\+\+R\+E\+G\+I\+O\+NS}\mbox{]}
\begin{DoxyCompactList}\small\item\em Boolean flag array to indicate the presence of a TL on sub-\/grid edges. \end{DoxyCompactList}\item 
std\+::vector$<$ int $>$ \hyperlink{class_mpi_manager_ad4a918a4cd19e644ff3295b2854fc6af}{local\+\_\+size}
\begin{DoxyCompactList}\small\item\em Dimensions of coarse lattice represented on this rank (includes halo). \end{DoxyCompactList}\item 
std\+::vector$<$ std\+::vector$<$ double $>$ $>$ \hyperlink{class_mpi_manager_a0211cd784c9ed1514d5968599e794313}{rank\+\_\+core\+\_\+edge}
\begin{DoxyCompactList}\small\item\em Absolute positions of edges of the core region represented on this rank. \end{DoxyCompactList}\item 
\hyperlink{struct_mpi_manager_1_1layer__edges}{layer\+\_\+edges} \hyperlink{class_mpi_manager_a0cb9f8f024ec0a186374995fb203ea1e}{sender\+\_\+layer\+\_\+pos}
\begin{DoxyCompactList}\small\item\em Structure containing sender layer edge positions. \end{DoxyCompactList}\item 
\hyperlink{struct_mpi_manager_1_1layer__edges}{layer\+\_\+edges} \hyperlink{class_mpi_manager_ad1ff57a97ec56efc1690dd3a5a52fd64}{recv\+\_\+layer\+\_\+pos}
\begin{DoxyCompactList}\small\item\em Structure containing receiver layer edge positions. \end{DoxyCompactList}\item 
\hyperlink{class_grid_obj}{Grid\+Obj} $\ast$ \hyperlink{class_mpi_manager_ad5ce72a2047a4cbb38f76d71c96571d8}{Grids}
\begin{DoxyCompactList}\small\item\em Pointer to grid hierarchy. \end{DoxyCompactList}\item 
std\+::vector$<$ std\+::vector$<$ double $>$ $>$ \hyperlink{class_mpi_manager_aafbb74832f69a915927b9bf252bd971d}{f\+\_\+buffer\+\_\+send}
\begin{DoxyCompactList}\small\item\em Array of resizeable outgoing buffers used for data transfer. \end{DoxyCompactList}\item 
std\+::vector$<$ std\+::vector$<$ double $>$ $>$ \hyperlink{class_mpi_manager_ab8f1eeab50fd4812b3a51af1a6c43713}{f\+\_\+buffer\+\_\+recv}
\begin{DoxyCompactList}\small\item\em Array of resizeable incoming buffers used for data transfer. \end{DoxyCompactList}\item 
M\+P\+I\+\_\+\+Status \hyperlink{class_mpi_manager_a257bc27e8099f1cbf5ac70b80d8eadaa}{recv\+\_\+stat}
\begin{DoxyCompactList}\small\item\em Status structure for Receive return information. \end{DoxyCompactList}\item 
M\+P\+I\+\_\+\+Request \hyperlink{class_mpi_manager_ae4ba6735840e949dff5cd63ab1695ff0}{send\+\_\+requests} \mbox{[}\hyperlink{definitions_8h_a144328eed4e90ebcf8a9f66aa7337266}{L\+\_\+\+M\+P\+I\+\_\+\+D\+I\+RS}\mbox{]}
\begin{DoxyCompactList}\small\item\em Array of request structures for handles to posted I\+Sends. \end{DoxyCompactList}\item 
M\+P\+I\+\_\+\+Status \hyperlink{class_mpi_manager_a3ccb49ceda719f0c6bb90593a880a730}{send\+\_\+stat} \mbox{[}\hyperlink{definitions_8h_a144328eed4e90ebcf8a9f66aa7337266}{L\+\_\+\+M\+P\+I\+\_\+\+D\+I\+RS}\mbox{]}
\begin{DoxyCompactList}\small\item\em Array of statuses for each Isend. \end{DoxyCompactList}\item 
std\+::vector$<$ \hyperlink{struct_mpi_manager_1_1buffer__struct}{buffer\+\_\+struct} $>$ \hyperlink{class_mpi_manager_a3a91c2e8cfb15027a0681c198f82d257}{buffer\+\_\+send\+\_\+info}
\begin{DoxyCompactList}\small\item\em Vectors of buffer\+\_\+info structures holding sender layer size info. \end{DoxyCompactList}\item 
std\+::vector$<$ \hyperlink{struct_mpi_manager_1_1buffer__struct}{buffer\+\_\+struct} $>$ \hyperlink{class_mpi_manager_a5e769fa077d24d62d10a9a0d303009d1}{buffer\+\_\+recv\+\_\+info}
\begin{DoxyCompactList}\small\item\em Vectors of buffer\+\_\+info structures holding receiver layer size info. \end{DoxyCompactList}\item 
std\+::ofstream $\ast$ \hyperlink{class_mpi_manager_a9a0dd93f57d78f568048197c95311832}{logout}
\begin{DoxyCompactList}\small\item\em Logfile handle. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Static Public Attributes}
\begin{DoxyCompactItemize}
\item 
static const int \hyperlink{class_mpi_manager_a0c5f58ce12a1a8002cb124bf61e80d09}{neighbour\+\_\+vectors} \mbox{[}3\mbox{]}\mbox{[}26\mbox{]}
\begin{DoxyCompactList}\small\item\em Cartesian unit vectors pointing to each neighbour in Cartesian topology. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
M\+PI Manager class. 

Class to manage all M\+PI apsects of the code. 

\subsection{Member Function Documentation}
\index{Mpi\+Manager@{Mpi\+Manager}!destroy\+Instance@{destroy\+Instance}}
\index{destroy\+Instance@{destroy\+Instance}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{destroy\+Instance()}{destroyInstance()}}]{\setlength{\rightskip}{0pt plus 5cm}void Mpi\+Manager\+::destroy\+Instance (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [static]}}\hypertarget{class_mpi_manager_a03b7914615ccb6e7b8a285f50860d503}{}\label{class_mpi_manager_a03b7914615ccb6e7b8a285f50860d503}


Instance destroyer. 

\index{Mpi\+Manager@{Mpi\+Manager}!get\+Instance@{get\+Instance}}
\index{get\+Instance@{get\+Instance}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{get\+Instance()}{getInstance()}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Mpi\+Manager} $\ast$ Mpi\+Manager\+::get\+Instance (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [static]}}\hypertarget{class_mpi_manager_a486e424ae1b9dfa3218d260b0f9a0a2f}{}\label{class_mpi_manager_a486e424ae1b9dfa3218d260b0f9a0a2f}


Instance creator. 

\index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+buffer\+\_\+pack@{mpi\+\_\+buffer\+\_\+pack}}
\index{mpi\+\_\+buffer\+\_\+pack@{mpi\+\_\+buffer\+\_\+pack}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+buffer\+\_\+pack(int dir, Grid\+Obj $\ast$g)}{mpi_buffer_pack(int dir, GridObj *g)}}]{\setlength{\rightskip}{0pt plus 5cm}void Mpi\+Manager\+::mpi\+\_\+buffer\+\_\+pack (
\begin{DoxyParamCaption}
\item[{int}]{dir, }
\item[{{\bf Grid\+Obj} $\ast$}]{g}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_ae1b33a4a24d9abf528528a296aa1d92d}{}\label{class_mpi_manager_ae1b33a4a24d9abf528528a296aa1d92d}


Method to pack the communication buffer. 

Communication buffer is packed with distribution values from the supplied grid. Amount of information is dictated by the direction of the communication being prepared.


\begin{DoxyParams}{Parameters}
{\em dir} & communication direction. \\
\hline
{\em g} & grid doing the communication. \\
\hline
\end{DoxyParams}
\index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+buffer\+\_\+size@{mpi\+\_\+buffer\+\_\+size}}
\index{mpi\+\_\+buffer\+\_\+size@{mpi\+\_\+buffer\+\_\+size}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+buffer\+\_\+size()}{mpi_buffer_size()}}]{\setlength{\rightskip}{0pt plus 5cm}void Mpi\+Manager\+::mpi\+\_\+buffer\+\_\+size (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_a1bf713399e26a5ffbc03147e0a20c585}{}\label{class_mpi_manager_a1bf713399e26a5ffbc03147e0a20c585}


Pre-\/calcualtion of the buffer sizes. 

Wrapper method for computing the buffer sizes for every grid on the rank, both sender and receiver. Must be called post-\/initialisation. \index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+buffer\+\_\+size\+\_\+recv@{mpi\+\_\+buffer\+\_\+size\+\_\+recv}}
\index{mpi\+\_\+buffer\+\_\+size\+\_\+recv@{mpi\+\_\+buffer\+\_\+size\+\_\+recv}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+buffer\+\_\+size\+\_\+recv(\+Grid\+Obj $\ast$\&g)}{mpi_buffer_size_recv(GridObj *&g)}}]{\setlength{\rightskip}{0pt plus 5cm}void Mpi\+Manager\+::mpi\+\_\+buffer\+\_\+size\+\_\+recv (
\begin{DoxyParamCaption}
\item[{{\bf Grid\+Obj} $\ast$\&}]{g}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_afa7547c05583bf6c52ea48cc1dc13336}{}\label{class_mpi_manager_afa7547c05583bf6c52ea48cc1dc13336}


Method to pre-\/compute the size of the receiver layer buffer. 

A halo consists of a receiver (outer) and sender (inner) layer. This method computes the size of the receiver layers in each communication direction (M\+PI directions).


\begin{DoxyParams}{Parameters}
{\em g} & grid being inspected. \\
\hline
\end{DoxyParams}
\index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+buffer\+\_\+size\+\_\+send@{mpi\+\_\+buffer\+\_\+size\+\_\+send}}
\index{mpi\+\_\+buffer\+\_\+size\+\_\+send@{mpi\+\_\+buffer\+\_\+size\+\_\+send}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+buffer\+\_\+size\+\_\+send(\+Grid\+Obj $\ast$\&g)}{mpi_buffer_size_send(GridObj *&g)}}]{\setlength{\rightskip}{0pt plus 5cm}void Mpi\+Manager\+::mpi\+\_\+buffer\+\_\+size\+\_\+send (
\begin{DoxyParamCaption}
\item[{{\bf Grid\+Obj} $\ast$\&}]{g}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_af26d2a0a2430f7c1b5def5f954a10f1d}{}\label{class_mpi_manager_af26d2a0a2430f7c1b5def5f954a10f1d}


Method to pre-\/compute the size of the sender layer buffer. 

A halo consists of a receiver (outer) and sender (inner) layer. This method computes the size of the sender layers in each communication direction (M\+PI directions).


\begin{DoxyParams}{Parameters}
{\em g} & grid being inspected. \\
\hline
\end{DoxyParams}
\index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+buffer\+\_\+unpack@{mpi\+\_\+buffer\+\_\+unpack}}
\index{mpi\+\_\+buffer\+\_\+unpack@{mpi\+\_\+buffer\+\_\+unpack}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+buffer\+\_\+unpack(int dir, Grid\+Obj $\ast$g)}{mpi_buffer_unpack(int dir, GridObj *g)}}]{\setlength{\rightskip}{0pt plus 5cm}void Mpi\+Manager\+::mpi\+\_\+buffer\+\_\+unpack (
\begin{DoxyParamCaption}
\item[{int}]{dir, }
\item[{{\bf Grid\+Obj} $\ast$}]{g}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_abf5e0511918b4ae6ec524d737618e341}{}\label{class_mpi_manager_abf5e0511918b4ae6ec524d737618e341}


Method to unpack the communication buffer. 

Communication buffer is unpacked onto the supplied grid. Amount and region of unpacking is dictated by the direction of the communication taking place.


\begin{DoxyParams}{Parameters}
{\em dir} & communication direction. \\
\hline
{\em g} & grid doing the communication. \\
\hline
\end{DoxyParams}
\index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+build\+Communicators@{mpi\+\_\+build\+Communicators}}
\index{mpi\+\_\+build\+Communicators@{mpi\+\_\+build\+Communicators}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+build\+Communicators()}{mpi_buildCommunicators()}}]{\setlength{\rightskip}{0pt plus 5cm}int Mpi\+Manager\+::mpi\+\_\+build\+Communicators (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_a749fa958cb7343183a69ca6191b45286}{}\label{class_mpi_manager_a749fa958cb7343183a69ca6191b45286}


Define writable sub-\/grid communicators. 

When using H\+D\+F5 in parallel, collective IO operations require all processes to write a non-\/zero amount of data to the same file. This method examines availability of sub-\/grid and writable data on the grid (if found) and ensures it is added to a new communicator. Must be called A\+F\+T\+ER the grids and buffers have been initialised. \index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+communicate@{mpi\+\_\+communicate}}
\index{mpi\+\_\+communicate@{mpi\+\_\+communicate}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+communicate(int level, int regnum)}{mpi_communicate(int level, int regnum)}}]{\setlength{\rightskip}{0pt plus 5cm}void Mpi\+Manager\+::mpi\+\_\+communicate (
\begin{DoxyParamCaption}
\item[{int}]{lev, }
\item[{int}]{reg}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_aedcf84c06fc3e0486fac61d09ce0a268}{}\label{class_mpi_manager_aedcf84c06fc3e0486fac61d09ce0a268}


Communication routine. 

This method implements the communication between grids of the same level and region across M\+PI processes. Each call effects communication in all valid directions for the grid of the supplied level and region.


\begin{DoxyParams}{Parameters}
{\em lev} & level of grid to communicate. \\
\hline
{\em reg} & region number of grid to communicate. \\
\hline
\end{DoxyParams}
\index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+get\+Opposite@{mpi\+\_\+get\+Opposite}}
\index{mpi\+\_\+get\+Opposite@{mpi\+\_\+get\+Opposite}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+get\+Opposite(int direction)}{mpi_getOpposite(int direction)}}]{\setlength{\rightskip}{0pt plus 5cm}int Mpi\+Manager\+::mpi\+\_\+get\+Opposite (
\begin{DoxyParamCaption}
\item[{int}]{direction}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_a3c10ab477c2e4387d6a02104f9b2a2ea}{}\label{class_mpi_manager_a3c10ab477c2e4387d6a02104f9b2a2ea}


Helper method to find opposite direction in M\+PI topology. 

The M\+PI directional vectors do not necessarily correspond to the lattice model direction. The M\+PI directional vectors are defined separately and hence there is a separate opposite finding method.


\begin{DoxyParams}{Parameters}
{\em direction} & the outgoing direction whose opposite you wish to find. \\
\hline
\end{DoxyParams}
\index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+gridbuild@{mpi\+\_\+gridbuild}}
\index{mpi\+\_\+gridbuild@{mpi\+\_\+gridbuild}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+gridbuild()}{mpi_gridbuild()}}]{\setlength{\rightskip}{0pt plus 5cm}void Mpi\+Manager\+::mpi\+\_\+gridbuild (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_a7f07e85131147b55eec643c791ec2ba0}{}\label{class_mpi_manager_a7f07e85131147b55eec643c791ec2ba0}


Domain decomposition. 

Method to decompose the domain and identify local grid sizes. Parameters defined here are used in \hyperlink{class_grid_obj}{Grid\+Obj} construction. \index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+init@{mpi\+\_\+init}}
\index{mpi\+\_\+init@{mpi\+\_\+init}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+init()}{mpi_init()}}]{\setlength{\rightskip}{0pt plus 5cm}void Mpi\+Manager\+::mpi\+\_\+init (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_a02adaa06e139dfca2bc71e1a1dbf25c7}{}\label{class_mpi_manager_a02adaa06e139dfca2bc71e1a1dbf25c7}


Initialisation routine. 

Method is responsible for initialising the M\+PI topolgy and associated data. Must be called immediately after M\+P\+I\+\_\+init(). For serial vuilds this gets called simply to intialise the M\+P\+IM with a basic set of grid information used by other methods. \index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+update\+Load\+Info@{mpi\+\_\+update\+Load\+Info}}
\index{mpi\+\_\+update\+Load\+Info@{mpi\+\_\+update\+Load\+Info}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+update\+Load\+Info()}{mpi_updateLoadInfo()}}]{\setlength{\rightskip}{0pt plus 5cm}void Mpi\+Manager\+::mpi\+\_\+update\+Load\+Info (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_ae6a8b1a857a00cdc4ed115512db050f4}{}\label{class_mpi_manager_ae6a8b1a857a00cdc4ed115512db050f4}


Update the load balancing information stored in the \hyperlink{class_mpi_manager}{Mpi\+Manager}. 

This method is executed by all processes. Counts the A\+C\+T\+I\+VE cells on the current rank and pushes the information to the master (rank 0) which writes this information to an output file if required. Must be called after the grids have been built or will return zero. \index{Mpi\+Manager@{Mpi\+Manager}!mpi\+\_\+writeout\+\_\+buf@{mpi\+\_\+writeout\+\_\+buf}}
\index{mpi\+\_\+writeout\+\_\+buf@{mpi\+\_\+writeout\+\_\+buf}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{mpi\+\_\+writeout\+\_\+buf(std\+::string filename, int dir)}{mpi_writeout_buf(std::string filename, int dir)}}]{\setlength{\rightskip}{0pt plus 5cm}void Mpi\+Manager\+::mpi\+\_\+writeout\+\_\+buf (
\begin{DoxyParamCaption}
\item[{std\+::string}]{filename, }
\item[{int}]{dir}
\end{DoxyParamCaption}
)}\hypertarget{class_mpi_manager_ab498bdf0822e2747f83c187d682dd934}{}\label{class_mpi_manager_ab498bdf0822e2747f83c187d682dd934}


Buffer A\+S\+C\+II writer. 

When verbose M\+PI logging is turned on this method will write out the communication buffer to an A\+S\+C\+II file. 

\subsection{Member Data Documentation}
\index{Mpi\+Manager@{Mpi\+Manager}!buffer\+\_\+recv\+\_\+info@{buffer\+\_\+recv\+\_\+info}}
\index{buffer\+\_\+recv\+\_\+info@{buffer\+\_\+recv\+\_\+info}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{buffer\+\_\+recv\+\_\+info}{buffer_recv_info}}]{\setlength{\rightskip}{0pt plus 5cm}std\+::vector$<${\bf buffer\+\_\+struct}$>$ Mpi\+Manager\+::buffer\+\_\+recv\+\_\+info}\hypertarget{class_mpi_manager_a5e769fa077d24d62d10a9a0d303009d1}{}\label{class_mpi_manager_a5e769fa077d24d62d10a9a0d303009d1}


Vectors of buffer\+\_\+info structures holding receiver layer size info. 

\index{Mpi\+Manager@{Mpi\+Manager}!buffer\+\_\+send\+\_\+info@{buffer\+\_\+send\+\_\+info}}
\index{buffer\+\_\+send\+\_\+info@{buffer\+\_\+send\+\_\+info}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{buffer\+\_\+send\+\_\+info}{buffer_send_info}}]{\setlength{\rightskip}{0pt plus 5cm}std\+::vector$<${\bf buffer\+\_\+struct}$>$ Mpi\+Manager\+::buffer\+\_\+send\+\_\+info}\hypertarget{class_mpi_manager_a3a91c2e8cfb15027a0681c198f82d257}{}\label{class_mpi_manager_a3a91c2e8cfb15027a0681c198f82d257}


Vectors of buffer\+\_\+info structures holding sender layer size info. 

\index{Mpi\+Manager@{Mpi\+Manager}!dimensions@{dimensions}}
\index{dimensions@{dimensions}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{dimensions}{dimensions}}]{\setlength{\rightskip}{0pt plus 5cm}int Mpi\+Manager\+::dimensions\mbox{[}{\bf L\+\_\+\+D\+I\+MS}\mbox{]}}\hypertarget{class_mpi_manager_a8d486f77671328cdc139f6cef2a4006f}{}\label{class_mpi_manager_a8d486f77671328cdc139f6cef2a4006f}


Size of M\+PI Cartesian topology. 

\index{Mpi\+Manager@{Mpi\+Manager}!f\+\_\+buffer\+\_\+recv@{f\+\_\+buffer\+\_\+recv}}
\index{f\+\_\+buffer\+\_\+recv@{f\+\_\+buffer\+\_\+recv}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{f\+\_\+buffer\+\_\+recv}{f_buffer_recv}}]{\setlength{\rightskip}{0pt plus 5cm}std\+::vector$<$ std\+::vector$<$double$>$ $>$ Mpi\+Manager\+::f\+\_\+buffer\+\_\+recv}\hypertarget{class_mpi_manager_ab8f1eeab50fd4812b3a51af1a6c43713}{}\label{class_mpi_manager_ab8f1eeab50fd4812b3a51af1a6c43713}


Array of resizeable incoming buffers used for data transfer. 

\index{Mpi\+Manager@{Mpi\+Manager}!f\+\_\+buffer\+\_\+send@{f\+\_\+buffer\+\_\+send}}
\index{f\+\_\+buffer\+\_\+send@{f\+\_\+buffer\+\_\+send}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{f\+\_\+buffer\+\_\+send}{f_buffer_send}}]{\setlength{\rightskip}{0pt plus 5cm}std\+::vector$<$ std\+::vector$<$double$>$ $>$ Mpi\+Manager\+::f\+\_\+buffer\+\_\+send}\hypertarget{class_mpi_manager_aafbb74832f69a915927b9bf252bd971d}{}\label{class_mpi_manager_aafbb74832f69a915927b9bf252bd971d}


Array of resizeable outgoing buffers used for data transfer. 

\index{Mpi\+Manager@{Mpi\+Manager}!global\+\_\+edges@{global\+\_\+edges}}
\index{global\+\_\+edges@{global\+\_\+edges}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{global\+\_\+edges}{global_edges}}]{\setlength{\rightskip}{0pt plus 5cm}double Mpi\+Manager\+::global\+\_\+edges\mbox{[}6\mbox{]}\mbox{[}{\bf L\+\_\+\+N\+U\+M\+\_\+\+L\+E\+V\+E\+LS} $\ast${\bf L\+\_\+\+N\+U\+M\+\_\+\+R\+E\+G\+I\+O\+NS}+1\mbox{]}}\hypertarget{class_mpi_manager_a26f0512e19009451431d6d0ba59bf81a}{}\label{class_mpi_manager_a26f0512e19009451431d6d0ba59bf81a}


Absolute position of grid edges (excluding halo of course). 

Since L0 can only be region = 0 this array should be accessed as \mbox{[}level + region\+\_\+number $\ast$ L\+\_\+\+N\+U\+M\+\_\+\+L\+E\+V\+E\+LS\mbox{]} in a loop where level cannot be 0. To retrieve L0 info, simply access \mbox{[}0\mbox{]}. The first index should be accessed using the enumerator e\+Cartesian\+Min\+Max. \index{Mpi\+Manager@{Mpi\+Manager}!global\+\_\+size@{global\+\_\+size}}
\index{global\+\_\+size@{global\+\_\+size}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{global\+\_\+size}{global_size}}]{\setlength{\rightskip}{0pt plus 5cm}int Mpi\+Manager\+::global\+\_\+size\mbox{[}3\mbox{]}\mbox{[}{\bf L\+\_\+\+N\+U\+M\+\_\+\+L\+E\+V\+E\+LS} $\ast${\bf L\+\_\+\+N\+U\+M\+\_\+\+R\+E\+G\+I\+O\+NS}+1\mbox{]}}\hypertarget{class_mpi_manager_a3cdf6e1ce19f22daa9e84bc88bf4382d}{}\label{class_mpi_manager_a3cdf6e1ce19f22daa9e84bc88bf4382d}


Overall size of each grid (excluding halo of course). 

Since L0 can only be region = 0 this array should be accessed as \mbox{[}level + region\+\_\+number $\ast$ L\+\_\+\+N\+U\+M\+\_\+\+L\+E\+V\+E\+LS\mbox{]} in a loop where level cannot be 0. To retrieve L0 info, simply access \mbox{[}0\mbox{]}. \index{Mpi\+Manager@{Mpi\+Manager}!Grids@{Grids}}
\index{Grids@{Grids}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{Grids}{Grids}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Grid\+Obj}$\ast$ Mpi\+Manager\+::\+Grids}\hypertarget{class_mpi_manager_ad5ce72a2047a4cbb38f76d71c96571d8}{}\label{class_mpi_manager_ad5ce72a2047a4cbb38f76d71c96571d8}


Pointer to grid hierarchy. 

\index{Mpi\+Manager@{Mpi\+Manager}!local\+\_\+size@{local\+\_\+size}}
\index{local\+\_\+size@{local\+\_\+size}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{local\+\_\+size}{local_size}}]{\setlength{\rightskip}{0pt plus 5cm}std\+::vector$<$int$>$ Mpi\+Manager\+::local\+\_\+size}\hypertarget{class_mpi_manager_ad4a918a4cd19e644ff3295b2854fc6af}{}\label{class_mpi_manager_ad4a918a4cd19e644ff3295b2854fc6af}


Dimensions of coarse lattice represented on this rank (includes halo). 

\index{Mpi\+Manager@{Mpi\+Manager}!logout@{logout}}
\index{logout@{logout}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{logout}{logout}}]{\setlength{\rightskip}{0pt plus 5cm}std\+::ofstream$\ast$ Mpi\+Manager\+::logout}\hypertarget{class_mpi_manager_a9a0dd93f57d78f568048197c95311832}{}\label{class_mpi_manager_a9a0dd93f57d78f568048197c95311832}


Logfile handle. 

\index{Mpi\+Manager@{Mpi\+Manager}!my\+\_\+rank@{my\+\_\+rank}}
\index{my\+\_\+rank@{my\+\_\+rank}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{my\+\_\+rank}{my_rank}}]{\setlength{\rightskip}{0pt plus 5cm}int Mpi\+Manager\+::my\+\_\+rank}\hypertarget{class_mpi_manager_a8329212abc23e5fa3e32e961b7823b5b}{}\label{class_mpi_manager_a8329212abc23e5fa3e32e961b7823b5b}


Rank number. 

\index{Mpi\+Manager@{Mpi\+Manager}!neighbour\+\_\+coords@{neighbour\+\_\+coords}}
\index{neighbour\+\_\+coords@{neighbour\+\_\+coords}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{neighbour\+\_\+coords}{neighbour_coords}}]{\setlength{\rightskip}{0pt plus 5cm}int Mpi\+Manager\+::neighbour\+\_\+coords\mbox{[}{\bf L\+\_\+\+D\+I\+MS}\mbox{]}\mbox{[}{\bf L\+\_\+\+M\+P\+I\+\_\+\+D\+I\+RS}\mbox{]}}\hypertarget{class_mpi_manager_a5a7268347fcab916adc61bee47e9f626}{}\label{class_mpi_manager_a5a7268347fcab916adc61bee47e9f626}


Coordinates in M\+PI topology of neighbour ranks. 

\index{Mpi\+Manager@{Mpi\+Manager}!neighbour\+\_\+rank@{neighbour\+\_\+rank}}
\index{neighbour\+\_\+rank@{neighbour\+\_\+rank}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{neighbour\+\_\+rank}{neighbour_rank}}]{\setlength{\rightskip}{0pt plus 5cm}int Mpi\+Manager\+::neighbour\+\_\+rank\mbox{[}{\bf L\+\_\+\+M\+P\+I\+\_\+\+D\+I\+RS}\mbox{]}}\hypertarget{class_mpi_manager_af2891954ff504c12ec6d5f845e906f28}{}\label{class_mpi_manager_af2891954ff504c12ec6d5f845e906f28}


Neighbour rank number for each direction in Cartesian topology. 

\index{Mpi\+Manager@{Mpi\+Manager}!neighbour\+\_\+vectors@{neighbour\+\_\+vectors}}
\index{neighbour\+\_\+vectors@{neighbour\+\_\+vectors}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{neighbour\+\_\+vectors}{neighbour_vectors}}]{\setlength{\rightskip}{0pt plus 5cm}const int Mpi\+Manager\+::neighbour\+\_\+vectors\hspace{0.3cm}{\ttfamily [static]}}\hypertarget{class_mpi_manager_a0c5f58ce12a1a8002cb124bf61e80d09}{}\label{class_mpi_manager_a0c5f58ce12a1a8002cb124bf61e80d09}
{\bfseries Initial value\+:}
\begin{DoxyCode}
= 
\{
    \{ 1, -1, 1, -1, 0, 0, -1, 1, 0, 0, 1, -1, 1, -1, 0, 0, -1, 1, -1, 1, -1, 1, 0, 0, 1, -1 \},
    \{ 0, 0, 1, -1, 1, -1, 1, -1, 0, 0, 0, 0, 1, -1, 1, -1, 1, -1, 0, 0, -1, 1, -1, 1, -1, 1 \},
    \{ 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1 \}
\}
\end{DoxyCode}


Cartesian unit vectors pointing to each neighbour in Cartesian topology. 

Define 3D such that first 8 mimic the 2D ones. Opposites are simply the next or previous column in the array. M\+S\+VC 2013 does not support initialiser lists tagged onto the constructor although it is valid C++ so I have had to make it static even though it goes against the idea of the singleton design. \index{Mpi\+Manager@{Mpi\+Manager}!num\+\_\+ranks@{num\+\_\+ranks}}
\index{num\+\_\+ranks@{num\+\_\+ranks}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{num\+\_\+ranks}{num_ranks}}]{\setlength{\rightskip}{0pt plus 5cm}int Mpi\+Manager\+::num\+\_\+ranks}\hypertarget{class_mpi_manager_af5156a5e4519f43230b6b84792464e48}{}\label{class_mpi_manager_af5156a5e4519f43230b6b84792464e48}


Total number of ranks in M\+PI Cartesian topology. 

\index{Mpi\+Manager@{Mpi\+Manager}!p\+\_\+data@{p\+\_\+data}}
\index{p\+\_\+data@{p\+\_\+data}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{p\+\_\+data}{p_data}}]{\setlength{\rightskip}{0pt plus 5cm}std\+::vector$<${\bf phdf5\+\_\+struct}$>$ Mpi\+Manager\+::p\+\_\+data}\hypertarget{class_mpi_manager_a03972530e718d5b0a7f119e9c6132179}{}\label{class_mpi_manager_a03972530e718d5b0a7f119e9c6132179}


Vector of structures containing halo descriptors for block writing (H\+D\+F5) 

\index{Mpi\+Manager@{Mpi\+Manager}!rank\+\_\+coords@{rank\+\_\+coords}}
\index{rank\+\_\+coords@{rank\+\_\+coords}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{rank\+\_\+coords}{rank_coords}}]{\setlength{\rightskip}{0pt plus 5cm}int Mpi\+Manager\+::rank\+\_\+coords\mbox{[}{\bf L\+\_\+\+D\+I\+MS}\mbox{]}}\hypertarget{class_mpi_manager_a54a3ad1d90d1508ebc82f81655d917f8}{}\label{class_mpi_manager_a54a3ad1d90d1508ebc82f81655d917f8}


Coordinates in M\+PI Cartesian topology. 

\index{Mpi\+Manager@{Mpi\+Manager}!rank\+\_\+core\+\_\+edge@{rank\+\_\+core\+\_\+edge}}
\index{rank\+\_\+core\+\_\+edge@{rank\+\_\+core\+\_\+edge}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{rank\+\_\+core\+\_\+edge}{rank_core_edge}}]{\setlength{\rightskip}{0pt plus 5cm}std\+::vector$<$ std\+::vector$<$double$>$ $>$ Mpi\+Manager\+::rank\+\_\+core\+\_\+edge}\hypertarget{class_mpi_manager_a0211cd784c9ed1514d5968599e794313}{}\label{class_mpi_manager_a0211cd784c9ed1514d5968599e794313}


Absolute positions of edges of the core region represented on this rank. 

Excludes outer overlapping layer (recv layer). Rows are x,y,z start and end pairs and columns are rank number. Access the rows using the e\+Cart\+Min\+Max enumeration. \index{Mpi\+Manager@{Mpi\+Manager}!recv\+\_\+layer\+\_\+pos@{recv\+\_\+layer\+\_\+pos}}
\index{recv\+\_\+layer\+\_\+pos@{recv\+\_\+layer\+\_\+pos}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{recv\+\_\+layer\+\_\+pos}{recv_layer_pos}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf layer\+\_\+edges} Mpi\+Manager\+::recv\+\_\+layer\+\_\+pos}\hypertarget{class_mpi_manager_ad1ff57a97ec56efc1690dd3a5a52fd64}{}\label{class_mpi_manager_ad1ff57a97ec56efc1690dd3a5a52fd64}


Structure containing receiver layer edge positions. 

\index{Mpi\+Manager@{Mpi\+Manager}!recv\+\_\+stat@{recv\+\_\+stat}}
\index{recv\+\_\+stat@{recv\+\_\+stat}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{recv\+\_\+stat}{recv_stat}}]{\setlength{\rightskip}{0pt plus 5cm}M\+P\+I\+\_\+\+Status Mpi\+Manager\+::recv\+\_\+stat}\hypertarget{class_mpi_manager_a257bc27e8099f1cbf5ac70b80d8eadaa}{}\label{class_mpi_manager_a257bc27e8099f1cbf5ac70b80d8eadaa}


Status structure for Receive return information. 

\index{Mpi\+Manager@{Mpi\+Manager}!send\+\_\+requests@{send\+\_\+requests}}
\index{send\+\_\+requests@{send\+\_\+requests}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{send\+\_\+requests}{send_requests}}]{\setlength{\rightskip}{0pt plus 5cm}M\+P\+I\+\_\+\+Request Mpi\+Manager\+::send\+\_\+requests\mbox{[}{\bf L\+\_\+\+M\+P\+I\+\_\+\+D\+I\+RS}\mbox{]}}\hypertarget{class_mpi_manager_ae4ba6735840e949dff5cd63ab1695ff0}{}\label{class_mpi_manager_ae4ba6735840e949dff5cd63ab1695ff0}


Array of request structures for handles to posted I\+Sends. 

\index{Mpi\+Manager@{Mpi\+Manager}!send\+\_\+stat@{send\+\_\+stat}}
\index{send\+\_\+stat@{send\+\_\+stat}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{send\+\_\+stat}{send_stat}}]{\setlength{\rightskip}{0pt plus 5cm}M\+P\+I\+\_\+\+Status Mpi\+Manager\+::send\+\_\+stat\mbox{[}{\bf L\+\_\+\+M\+P\+I\+\_\+\+D\+I\+RS}\mbox{]}}\hypertarget{class_mpi_manager_a3ccb49ceda719f0c6bb90593a880a730}{}\label{class_mpi_manager_a3ccb49ceda719f0c6bb90593a880a730}


Array of statuses for each Isend. 

\index{Mpi\+Manager@{Mpi\+Manager}!sender\+\_\+layer\+\_\+pos@{sender\+\_\+layer\+\_\+pos}}
\index{sender\+\_\+layer\+\_\+pos@{sender\+\_\+layer\+\_\+pos}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{sender\+\_\+layer\+\_\+pos}{sender_layer_pos}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf layer\+\_\+edges} Mpi\+Manager\+::sender\+\_\+layer\+\_\+pos}\hypertarget{class_mpi_manager_a0cb9f8f024ec0a186374995fb203ea1e}{}\label{class_mpi_manager_a0cb9f8f024ec0a186374995fb203ea1e}


Structure containing sender layer edge positions. 

\index{Mpi\+Manager@{Mpi\+Manager}!sub\+Grid\+\_\+comm@{sub\+Grid\+\_\+comm}}
\index{sub\+Grid\+\_\+comm@{sub\+Grid\+\_\+comm}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{sub\+Grid\+\_\+comm}{subGrid_comm}}]{\setlength{\rightskip}{0pt plus 5cm}M\+P\+I\+\_\+\+Comm Mpi\+Manager\+::sub\+Grid\+\_\+comm\mbox{[}{\bf L\+\_\+\+N\+U\+M\+\_\+\+L\+E\+V\+E\+LS} $\ast${\bf L\+\_\+\+N\+U\+M\+\_\+\+R\+E\+G\+I\+O\+NS}\mbox{]}}\hypertarget{class_mpi_manager_a0926101699de914f6be018885bea25b1}{}\label{class_mpi_manager_a0926101699de914f6be018885bea25b1}


Communicators for sub-\/grid / region combinations. 

\index{Mpi\+Manager@{Mpi\+Manager}!subgrid\+\_\+tlayer\+\_\+key@{subgrid\+\_\+tlayer\+\_\+key}}
\index{subgrid\+\_\+tlayer\+\_\+key@{subgrid\+\_\+tlayer\+\_\+key}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{subgrid\+\_\+tlayer\+\_\+key}{subgrid_tlayer_key}}]{\setlength{\rightskip}{0pt plus 5cm}bool Mpi\+Manager\+::subgrid\+\_\+tlayer\+\_\+key\mbox{[}6\mbox{]}\mbox{[}{\bf L\+\_\+\+N\+U\+M\+\_\+\+L\+E\+V\+E\+LS} $\ast${\bf L\+\_\+\+N\+U\+M\+\_\+\+R\+E\+G\+I\+O\+NS}\mbox{]}}\hypertarget{class_mpi_manager_a18d53c4f9968cccec36127d33776e3d4}{}\label{class_mpi_manager_a18d53c4f9968cccec36127d33776e3d4}


Boolean flag array to indicate the presence of a TL on sub-\/grid edges. 

It is not a given that a sub-\/grid has a TL on every edge of the grid. Specifically if we have a sub-\/grid which is perodic (or in future, which merges with another sub-\/grid?). The H\+D\+F5 writer needs to know whether to exclude sites to account for TL or not so we store information here from the sub-\/grid initialisation. The first index should be accessed using the enumerator e\+Cartesian\+Min\+Max. If no sub-\/grids present then adopts a default 6x1 size. \index{Mpi\+Manager@{Mpi\+Manager}!world\+\_\+comm@{world\+\_\+comm}}
\index{world\+\_\+comm@{world\+\_\+comm}!Mpi\+Manager@{Mpi\+Manager}}
\subsubsection[{\texorpdfstring{world\+\_\+comm}{world_comm}}]{\setlength{\rightskip}{0pt plus 5cm}M\+P\+I\+\_\+\+Comm Mpi\+Manager\+::world\+\_\+comm}\hypertarget{class_mpi_manager_aec1ed834d1a8fa19f87499fb0d5cd332}{}\label{class_mpi_manager_aec1ed834d1a8fa19f87499fb0d5cd332}


Global M\+PI communicator. 



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
\hyperlink{_mpi_manager_8h}{Mpi\+Manager.\+h}\item 
\hyperlink{_mpi__buffer__pack_8cpp}{Mpi\+\_\+buffer\+\_\+pack.\+cpp}\item 
\hyperlink{_mpi__buffer__size__recv_8cpp}{Mpi\+\_\+buffer\+\_\+size\+\_\+recv.\+cpp}\item 
\hyperlink{_mpi__buffer__size__send_8cpp}{Mpi\+\_\+buffer\+\_\+size\+\_\+send.\+cpp}\item 
\hyperlink{_mpi__buffer__unpk_8cpp}{Mpi\+\_\+buffer\+\_\+unpk.\+cpp}\item 
\hyperlink{_mpi_manager_8cpp}{Mpi\+Manager.\+cpp}\end{DoxyCompactItemize}
